# You should remove whichever example you are not using.

## watsonx.ai Example
IS_WATSONX="True"
IS_AZURE="False"
LLM_TEMPERATURE="0.0"
IS_NATIVE_FUNCTION_CALLING_SUPPORTED="True"
LLM_MODEL_NAME="ibm/granite-3-8b-instruct"
LLM_CONFIGURATION_PARAMETERS='{"temperature": 0.0, "max_new_tokens": 8192, "decoding_method": "greedy", "random_seed":42, "top_p":0.0000001}'
LLM_BASE_URL="" # e.g. https://us-south.ml.cloud.ibm.com
LLM_API_KEY="" # IBM Cloud IAM API Key
LLM_PROJECT_ID=""

## Azure AI Foundry Example
IS_AZURE="True"
IS_WATSONX="False"
LLM_TEMPERATURE="0.0"
IS_NATIVE_FUNCTION_CALLING_SUPPORTED="True"
LLM_API_KEY="" # This is the Endpoint->Key under your model deployment
LLM_MODEL_NAME="" # Change the model name here e.g. gpt-4o
LLM_BASE_URL="" # This is the Endpoint->Target URI under your model deployment
LLM_API_VERSION="" # This is at the end of the BASE_URL i.e. ?api-version=2024-08-01-preview, you only need the last part 2024-08-01-preview
LLM_SEED=42
LLM_TOP_P=0.0000001

AGENT_TASK_DIRECTORY="config"

GOD_MODE="False" # Setting this to "True" will disable safeguards during remediation step

GRAFANA_URL="" # URL to be used for Grafana API calls
TOPOLOGY_URL="" # Same as grafana URL
GRAFANA_SERVICE_ACCOUNT_TOKEN="" # Create this in the Grafana dashboard
KUBECONFIG="" # KUBECONFIG of your local or remote cluster
STRUCTURED_UNSTRUCTURED_OUTPUT_DIRECTORY_PATH="" # Path for output files
